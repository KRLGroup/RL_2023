{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "691171dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable_baselines3 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.2.1)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stable_baselines3) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stable_baselines3) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stable_baselines3) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stable_baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stable_baselines3) (2.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stable_baselines3) (3.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->stable_baselines3) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->stable_baselines3) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->stable_baselines3) (10.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->stable_baselines3) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->stable_baselines3) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->stable_baselines3) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\michele\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b09a0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d945059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define env\n",
    "env_id = \"CartPole-v1\"\n",
    "#env_id = \"Acrobot-v1\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3daca912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 1847     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.1        |\n",
      "|    ep_rew_mean          | 27.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1331        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008492012 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.0019     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.05        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 55.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.2        |\n",
      "|    ep_rew_mean          | 37.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1252        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007122593 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0662      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 37.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.9        |\n",
      "|    ep_rew_mean          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1234        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008285938 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 56          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62.4        |\n",
      "|    ep_rew_mean          | 62.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1224        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006378752 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 67.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 76.3      |\n",
      "|    ep_rew_mean          | 76.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 1215      |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 12288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0095693 |\n",
      "|    clip_fraction        | 0.0762    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.594    |\n",
      "|    explained_variance   | 0.326     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 33        |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | -0.0162   |\n",
      "|    value_loss           | 73.3      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90.1        |\n",
      "|    ep_rew_mean          | 90.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1210        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005556739 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.599      |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00831    |\n",
      "|    value_loss           | 49          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | 108         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1206        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006387356 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.21        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00858    |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 127          |\n",
      "|    ep_rew_mean          | 127          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1207         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061047063 |\n",
      "|    clip_fraction        | 0.0603       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.593       |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.7          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00851     |\n",
      "|    value_loss           | 24.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 141         |\n",
      "|    ep_rew_mean          | 141         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1209        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009146268 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.7        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00834    |\n",
      "|    value_loss           | 49          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 152         |\n",
      "|    ep_rew_mean          | 152         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1211        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009648493 |\n",
      "|    clip_fraction        | 0.0967      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 71.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 171          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1211         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048903855 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.568       |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00701     |\n",
      "|    value_loss           | 55.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 188          |\n",
      "|    ep_rew_mean          | 188          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1210         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047593582 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.553        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    value_loss           | 7.87         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 205         |\n",
      "|    ep_rew_mean          | 205         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1212        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011904197 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.29        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 4.37        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 223          |\n",
      "|    ep_rew_mean          | 223          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1214         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063250316 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.216        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    value_loss           | 3.19         |\n",
      "------------------------------------------\n",
      "Mean reward expert agent= 500.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "#define expert agent\n",
    "ppo_expert = PPO('MlpPolicy', env_id, verbose=1)\n",
    "\n",
    "#train expert\n",
    "ppo_expert.learn(total_timesteps=3e4)\n",
    "\n",
    "#save expert\n",
    "ppo_expert.save(\"ppo_expert\")\n",
    "\n",
    "#evaluate expert\n",
    "mean_reward, std_reward = evaluate_policy(ppo_expert, Monitor(env), n_eval_episodes=10)\n",
    "print(f\"Mean reward expert agent= {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "480991d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 4)\n",
      "(40000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:09<00:00, 4179.79it/s]\n"
     ]
    }
   ],
   "source": [
    "##create expert dataset\n",
    "\n",
    "#empty dataset\n",
    "num_interactions = int(4e4)\n",
    "\n",
    "expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
    "expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
    "\n",
    "print(expert_observations.shape)\n",
    "print(expert_actions.shape)\n",
    "\n",
    "#collect experience usign expert policy\n",
    "obs, _ = env.reset()\n",
    "for i in tqdm(range(num_interactions)):\n",
    "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
    "    expert_observations[i] = obs\n",
    "    expert_actions[i] = action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05a75579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset\n",
    "np.savez_compressed(\n",
    "   \"expert_data\",\n",
    "   expert_actions=expert_actions,\n",
    "   expert_observations=expert_observations,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce03945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dataset class\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11af5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "#split in 80% training and 20%test\n",
    "batch_size = 64\n",
    "train_prop = 0.8\n",
    "train_size = int(train_prop * len(expert_dataset))\n",
    "test_size = len(expert_dataset) - train_size\n",
    "train_expert_dataset, test_expert_dataset = random_split(expert_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = th.utils.data.DataLoader(  dataset=train_expert_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = th.utils.data.DataLoader(  dataset=test_expert_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc218b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_frames_as_gif(frames, path='./', filename='Behavioral_cloning.gif'):\n",
    "\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c7a5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda:  False\n"
     ]
    }
   ],
   "source": [
    "###### Define student agent\n",
    "no_cuda = False\n",
    "use_cuda = not no_cuda and th.cuda.is_available()\n",
    "print('use_cuda: ', use_cuda)\n",
    "   \n",
    "device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class StudentAgent:\n",
    "    def __init__(self, env, train_loader, test_loader, learning_rate):\n",
    "        self.env = env\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        \n",
    "        n_inputs = env.observation_space.shape[0]\n",
    "        n_outputs = env.action_space.n\n",
    "        \n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 16), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, n_outputs),\n",
    "            nn.Softmax(dim=-1))\n",
    "        \n",
    "        print(\"policy net: \", self.policy)\n",
    "        \n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer =  optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.num_eval_episodes = 10\n",
    "        \n",
    "    def train(self, num_epochs):\n",
    "        self.policy.train()\n",
    "        self.policy.to(device)\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                obs, expert_action = data.to(device), target.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                obs = obs.float()\n",
    "                student_action = self.policy(obs)\n",
    "                expert_action = expert_action.long()\n",
    "                loss = self.loss_criterion(student_action, expert_action)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            #compute accuracy\n",
    "            train_acc = self.compute_accuracy(self.train_loader)\n",
    "            test_acc = self.compute_accuracy(self.test_loader)\n",
    "            policy_return = self.evaluate_policy(self.num_eval_episodes)\n",
    "            print(\"Epoch {}:\\ttrain accuracy: {}\\ttest accuracy: {}\\tpolicy return:{}\".format(epoch, train_acc, test_acc, policy_return))\n",
    "\n",
    "    def compute_accuracy(self, loader):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        self.policy.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                obs, expert_action = data.to(device), target.to(device)\n",
    "                obs = obs.float()\n",
    "            \n",
    "                student_action = self.policy_action(obs)\n",
    "            \n",
    "                total += student_action.size()[0]\n",
    "                correct += sum(student_action==expert_action).item()\n",
    "            \n",
    "        accuracy = 100. * correct/(float)(total)\n",
    "            \n",
    "        return accuracy\n",
    "            \n",
    "        \n",
    "    \n",
    "    def policy_action(self, obs):\n",
    "        obs = obs.to(device)\n",
    "        policy_act = self.policy(obs)\n",
    "        return th.argmax(policy_act, dim= 1)\n",
    "        \n",
    "    def evaluate_policy(self, num_episodes, render=False):\n",
    "        if render:\n",
    "            env = gym.make(self.env.spec.id, render_mode='rgb_array')\n",
    "        else:\n",
    "            env = self.env\n",
    "        rewards = []\n",
    "        \n",
    "        for ep in range(num_episodes):\n",
    "            done = False\n",
    "            tot_rew = 0\n",
    "            obs, _ = env.reset()\n",
    "            frames_gif=[]\n",
    "            while not done:\n",
    "                \n",
    "                obs = th.FloatTensor(obs).unsqueeze(0)\n",
    "                action = self.policy_action(obs)\n",
    "                obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "                done = terminated or truncated\n",
    "                if render==True and ep==num_episodes-1:\n",
    "                    frames_gif.append(env.render())\n",
    "                tot_rew += reward\n",
    "            rewards.append(tot_rew)\n",
    "        if render:\n",
    "            save_frames_as_gif(frames_gif)\n",
    "        return mean(rewards)\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "614e3ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy net:  Sequential(\n",
      "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (3): Softmax(dim=-1)\n",
      ")\n",
      "Epoch 0:\ttrain accuracy: 87.3375\ttest accuracy: 87.3375\tpolicy return:500.0\n",
      "Epoch 1:\ttrain accuracy: 94.475\ttest accuracy: 94.475\tpolicy return:500.0\n",
      "Epoch 2:\ttrain accuracy: 95.5625\ttest accuracy: 95.5625\tpolicy return:500.0\n",
      "Epoch 3:\ttrain accuracy: 95.9125\ttest accuracy: 95.9125\tpolicy return:500.0\n",
      "Epoch 4:\ttrain accuracy: 96.225\ttest accuracy: 96.225\tpolicy return:500.0\n",
      "Epoch 5:\ttrain accuracy: 96.3625\ttest accuracy: 96.3625\tpolicy return:500.0\n",
      "Epoch 6:\ttrain accuracy: 96.7\ttest accuracy: 96.7\tpolicy return:500.0\n",
      "Epoch 7:\ttrain accuracy: 96.6375\ttest accuracy: 96.6375\tpolicy return:500.0\n",
      "Epoch 8:\ttrain accuracy: 97.4625\ttest accuracy: 97.4625\tpolicy return:500.0\n",
      "Epoch 9:\ttrain accuracy: 97.575\ttest accuracy: 97.575\tpolicy return:500.0\n",
      "Epoch 10:\ttrain accuracy: 97.3125\ttest accuracy: 97.3125\tpolicy return:500.0\n",
      "Epoch 11:\ttrain accuracy: 98.05\ttest accuracy: 98.05\tpolicy return:500.0\n",
      "Epoch 12:\ttrain accuracy: 98.175\ttest accuracy: 98.175\tpolicy return:500.0\n",
      "Epoch 13:\ttrain accuracy: 98.45\ttest accuracy: 98.45\tpolicy return:500.0\n",
      "Epoch 14:\ttrain accuracy: 98.05\ttest accuracy: 98.05\tpolicy return:500.0\n",
      "Epoch 15:\ttrain accuracy: 97.8625\ttest accuracy: 97.8625\tpolicy return:500.0\n",
      "Epoch 16:\ttrain accuracy: 98.475\ttest accuracy: 98.475\tpolicy return:500.0\n",
      "Epoch 17:\ttrain accuracy: 96.5625\ttest accuracy: 96.5625\tpolicy return:500.0\n",
      "Epoch 18:\ttrain accuracy: 98.85\ttest accuracy: 98.85\tpolicy return:500.0\n",
      "Epoch 19:\ttrain accuracy: 97.425\ttest accuracy: 97.425\tpolicy return:500.0\n",
      "Epoch 20:\ttrain accuracy: 99.0125\ttest accuracy: 99.0125\tpolicy return:500.0\n",
      "Epoch 21:\ttrain accuracy: 99.0625\ttest accuracy: 99.0625\tpolicy return:500.0\n",
      "Epoch 22:\ttrain accuracy: 98.8375\ttest accuracy: 98.8375\tpolicy return:500.0\n",
      "Epoch 23:\ttrain accuracy: 97.75\ttest accuracy: 97.75\tpolicy return:500.0\n",
      "Epoch 24:\ttrain accuracy: 99.175\ttest accuracy: 99.175\tpolicy return:500.0\n",
      "Epoch 25:\ttrain accuracy: 99.175\ttest accuracy: 99.175\tpolicy return:500.0\n",
      "Epoch 26:\ttrain accuracy: 99.075\ttest accuracy: 99.075\tpolicy return:500.0\n",
      "Epoch 27:\ttrain accuracy: 98.825\ttest accuracy: 98.825\tpolicy return:500.0\n",
      "Epoch 28:\ttrain accuracy: 98.8875\ttest accuracy: 98.8875\tpolicy return:500.0\n",
      "Epoch 29:\ttrain accuracy: 98.95\ttest accuracy: 98.95\tpolicy return:500.0\n",
      "Epoch 30:\ttrain accuracy: 97.1875\ttest accuracy: 97.1875\tpolicy return:500.0\n",
      "Epoch 31:\ttrain accuracy: 95.875\ttest accuracy: 95.875\tpolicy return:500.0\n",
      "Epoch 32:\ttrain accuracy: 99.325\ttest accuracy: 99.325\tpolicy return:500.0\n",
      "Epoch 33:\ttrain accuracy: 98.9\ttest accuracy: 98.9\tpolicy return:500.0\n",
      "Epoch 34:\ttrain accuracy: 99.2125\ttest accuracy: 99.2125\tpolicy return:500.0\n",
      "Epoch 35:\ttrain accuracy: 99.4\ttest accuracy: 99.4\tpolicy return:500.0\n",
      "Epoch 36:\ttrain accuracy: 99.0875\ttest accuracy: 99.0875\tpolicy return:500.0\n",
      "Epoch 37:\ttrain accuracy: 98.8875\ttest accuracy: 98.8875\tpolicy return:500.0\n",
      "Epoch 38:\ttrain accuracy: 98.2\ttest accuracy: 98.2\tpolicy return:500.0\n",
      "Epoch 39:\ttrain accuracy: 98.9875\ttest accuracy: 98.9875\tpolicy return:500.0\n",
      "Epoch 40:\ttrain accuracy: 99.0\ttest accuracy: 99.0\tpolicy return:500.0\n",
      "Epoch 41:\ttrain accuracy: 98.775\ttest accuracy: 98.775\tpolicy return:500.0\n",
      "Epoch 42:\ttrain accuracy: 99.0375\ttest accuracy: 99.0375\tpolicy return:500.0\n",
      "Epoch 43:\ttrain accuracy: 99.3125\ttest accuracy: 99.3125\tpolicy return:500.0\n",
      "Epoch 44:\ttrain accuracy: 98.95\ttest accuracy: 98.95\tpolicy return:500.0\n",
      "Epoch 45:\ttrain accuracy: 98.075\ttest accuracy: 98.075\tpolicy return:500.0\n",
      "Epoch 46:\ttrain accuracy: 99.4125\ttest accuracy: 99.4125\tpolicy return:500.0\n",
      "Epoch 47:\ttrain accuracy: 99.1\ttest accuracy: 99.1\tpolicy return:500.0\n",
      "Epoch 48:\ttrain accuracy: 98.1375\ttest accuracy: 98.1375\tpolicy return:500.0\n",
      "Epoch 49:\ttrain accuracy: 97.85\ttest accuracy: 97.85\tpolicy return:500.0\n"
     ]
    }
   ],
   "source": [
    "student = StudentAgent(env, train_loader, test_loader, 0.01)\n",
    "student.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00442cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student.evaluate_policy(1, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
